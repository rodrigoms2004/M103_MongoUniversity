Shardes Cluster

Mongos process, use metadata in config servers replica set


Elements

- Shards, store distributed collections
- Config Server, store metadata about each shard
- Mongos, routes queriews to shards


################################################################
When to Shard

do not shard if...
- is possible to impruve current servers
- 10x RAM do not cost 100x $

the dead end...
Horizontal Scale

usually servers has between 2TB to 5TB of data

Sharding offers:
1 - Single Thread Operations
    - Aggregation Pipeline Commands, be  careful about pipeline!
    
2 - Geographical Distributed Data
    Zone Sharding (out of scope)

################################################################

Correct answers:

When we reach the most powerful servers available, maximizing our vertical scale options.
    Sharding can provide an alternative to vertical scaling.

Data sovereignty laws require data to be located in a specific geography.
    Sharding allows us to store different pieces of data in specific countries or regions.

When holding more than 5TB per server and operational costs increase dramatically.
    Generally, when our deployment reaches 2-5TB per server, we should consider sharding.


Incorrect answers:

When we start a new project with MongoDB.
    We should carefully consider if we need to have a sharding system out of the start. 
    This might be required for some projects, but certainly not always the ideal moment to address scalability needs.

When our server disks are full.
    Maxing out the capacity of our disks is not a reason for sharding. 
    Scaling up might make more sense than to add complexity to our system.

################################################################
Sharding Architecture

                                            |----------->Shard1
Client App ------------> Mongos ------------|----------->Shard2
                                            |----------->Shard3


Names divides by...

Shard1, last names A-J
Shard2, last names K-Q
Shard3, last names R-Z

Could be multiple mongos 

Sharded Cluster metadata, stored in Config Servers


Config Servers
    Shard           Data
    1               A-J
    2               K-Q
    3               R-Z


                                            |----------->Shard1
Client App ------------> Mongos ------------|----------->Shard2
                            |               |----------->Shard3
                            |
                        Config Servers


Config Server keep balance between Shards nodes

In the primary shard are made Aggregation operations

SHARD_MERGE takes place in Mongos


################################################################
Setting Up a Sharded Cluster

Shard1 (three nodes)

CSRS (three nodes)

# csrs_1.conf
sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26001
systemLog:
  destination: file
  path: /var/mongodb/db/csrs1.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs1

################################################################
# csrs_2.conf
sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26002
systemLog:
  destination: file
  path: /var/mongodb/db/csrs2.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs2

################################################################
# csrs_3.conf
sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26003
systemLog:
  destination: file
  path: /var/mongodb/db/csrs3.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs3


################################################################
# creating db folders

mkdir /var/mongodb/db/{csrs1,csrs2,csrs3}
################################################################
Starting the three config servers:

mongod -f csrs_1.conf
mongod -f csrs_2.conf
mongod -f csrs_3.conf
################################################################
# Connect to one of the config servers
mongo --port 26001

################################################################
# Initiating the CSRS:

rs.initiate()

################################################################
# Creating super user on CSRS:
use admin
db.createUser({
  user: "m103-admin",
  pwd: "m103-pass",
  roles: [
    {role: "root", db: "admin"}
  ]
})
################################################################
# Authenticating as the super user:

db.auth("m103-admin", "m103-pass")

################################################################
# Add the second and third node to the CSRS:

rs.add("192.168.103.100:26002")
rs.add("192.168.103.100:26003")

################################################################
# Connecting to the cluster

mongo --host "m103-csrs/192.168.103.100:26001" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

################################################################
# Connecting to each node

mongo --host "192.168.103.100:26001" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"
mongo --host "192.168.103.100:26002" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"
mongo --host "192.168.103.100:26003" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

################################################################
# Mongos config (mongos.conf)

sharding:
  configDB: m103-csrs/192.168.103.100:26001,192.168.103.100:26002,192.168.103.100:26003
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26000
systemLog:
  destination: file
  path: /var/mongodb/db/mongos.log
  logAppend: true
processManagement:
  fork: true


################################################################
# run mongos 

mongos -f mongos.conf

################################################################
# Connect to mongos

mongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin

MongoDB shell version v3.6.12
connecting to: mongodb://127.0.0.1:26000/?authSource=admin&gssapiServiceName=mongodb
Implicit session: session { "id" : UUID("2307e517-aa9b-4629-83cb-7f0fdeb1d361") }
MongoDB server version: 3.6.12


################################################################
# Check sharding status:

sh.status()
--- Sharding Status --- 
  sharding version: {
  	"_id" : 1,
  	"minCompatibleVersion" : 5,
  	"currentVersion" : 6,
  	"clusterId" : ObjectId("5cc62d5eb15caac4aa6fbe93")
  }
  shards:
  active mongoses:
        "3.6.12" : 1
  autosplit:
        Currently enabled: yes
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours: 
                No recent migrations
  databases:
        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }

################################################################
# Updated configuration for node1.conf:

sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node1
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27011
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node1/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

################################################################
# Updated configuration for node2.conf:

sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node2
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27012
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node2/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

################################################################
# Updated configuration for node3.conf:

sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node3
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27013
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node3/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

################################################################
# Starting nodes from replicaSet folder

cd ../replicaSet/
mongod -f node1.conf
mongod -f node2.conf
mongod -f node3.conf

# Checkig it
mongo --host "m103-example/192.168.103.100:27011" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

# come back to shardedCluster folder
cd ../shardedCluster/

################################################################
# Connecting directly to secondary node (note that if an election has taken place in your replica set, 
the specified node may have become primary):

# connect to a secondary node

mongo --port 27012 -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"


################################################################
# Shutting down node:
use admin
db.shutdownServer()

################################################################
# Restarting node with new configuration:
mongod -f node2.conf

# do the same with node3
mongo --port 27013 -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

use admin
db.shutdownServer()

mongod -f node3.conf

################################################################
# Connect to the last one 

mongo --port 27011 -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

################################################################
# Stepping down current primary:

rs.stepDown()

# shut it down and run new config
use admin
db.shutdownServer()

mongod -f node1.conf

################################################################
# Connect to the mongos

mongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin

################################################################
# Adding new shard to cluster from mongos:

sh.addShard("m103-example/192.168.103.100:27013")

{
	"shardAdded" : "m103-example",
	"ok" : 1,
	"operationTime" : Timestamp(1556498029, 4),
	"$clusterTime" : {
		"clusterTime" : Timestamp(1556498029, 4),
		"signature" : {
			"hash" : BinData(0,"4D3z83YxpGPuRy/12MSJuZWR6cU="),
			"keyId" : NumberLong("6685080578628255770")
		}
	}
}


################################################################
# checking status

sh.status()

--- Sharding Status --- 
  sharding version: {
  	"_id" : 1,
  	"minCompatibleVersion" : 5,
  	"currentVersion" : 6,
  	"clusterId" : ObjectId("5cc62d5eb15caac4aa6fbe93")
  }
  shards:
        {  "_id" : "m103-example",  "host" : "m103-example/192.168.103.100:27011,192.168.103.100:27012,192.168.103.100:27013",  "state" : 1 }
  active mongoses:
        "3.6.12" : 1
  autosplit:
        Currently enabled: yes
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours: 
                No recent migrations
  databases:
        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }

################################################################
# Config DB

# connect to the mongos
mongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin

# Switch to config DB:
use config

# Query config.databases:
db.databases.find().pretty()

# Query config.collections:
db.collections.find().pretty()

# Query config.shards:
db.shards.find().pretty()

# Query config.chunks:
db.chunks.find().pretty()

# Query config.mongos:
db.mongos.find().pretty()

# RECAP
- Config DB: shards, chunks, mongos
- when to write data to Config DB: RARELY!

################################################################
# Shard keys

# example
{ x: 1 }
Logical groups: 1 <= x < 3    3 <= x < 6  6 <= x < 9

# SHARD KEY 

- Shard key fields must be indexed
  Indexes must exist FIRST before you can select the indexed fields for your shard key

- Shard Keys are immutable
  You cannot change the shard key fields post-sharding
  You cannot change the values of the shard key fields post-sharding
  
- Shard Keys are permanent
  You cannot unshard a sharded collection


# Show collections in m103 database:
use m103
show collections

# Enable sharding on the m103 database:
sh.enableSharding("m103")

# Find one document from the products collection, to help us choose a shard key:
db.products.findOne()

# Create an index on sku:
db.products.createIndex( { "sku" : 1 } )

# Shard the products collection on sku:
sh.shardCollection("m103.products", {"sku" : 1 } )

# Checking the status of the sharded cluster:
sh.status()

# RECAP
- Shard keys determine data distribution in a sharded cluster
- Shard keys are immutable
- Shard key value are immutable
- Create the underlying idex first before sharding on the indexed field or fields

################################################################
# Picking a Good Shard Keys

# YOU SHARD COLLECTIONS, NOT databases


# What makes a Good Shard Key?

The goal is a shard key whose values provides good write distribution 
- Cardinality
- Frequency
- Monotonic Change

# Cardinality
  High Cardinality = many possible unique shard key values

# Frequency
  Low Frequency = low repetition of a given unique shard key value

# Type of Change, Avoid shard keys that change monotonically

The goal is a shard key whose values provides good write distribution 
- High Cardinality (lots of unique possible values)
- Low Frequency (very little repetition of those unique values)
- Non-Monotonically Changing (non-linear change in values)

# Read Isolation 


################################################################
# Hashed Shard Keys

{ x: 3 } ---> HashFunction(x) ---> data shard 

Hashed Shard Key Considerations
- Queries on ranges of shard key values are more likely to be scatter-gather
- Cannot support geographically isolated read operations using zoned sharding
- Hashed Index must be on a single non-array field
- Hashed Indexes don't support fast sorting

# Creating a hashed shard key
1 - Use sh.enableSharding("<database>") to enable sharding for the specified database
2 - Use db.collection.createIndex({ "<field>": "hashed" }) to create a index for your shard key fields
3 - Use
  sh.shardCollection(
    "<database>.<collection>", { <shard key field> : "hashed" }
  )
  to shard the collection

# RECAP
- Even distribution of shard keys on monotonically changing fields like dates
- No fast sorts, targeted queries on ranges of shard key values, or geographically isolated workloads
- Hashed indexes are single field, non-array

################################################################
# CHUNKS

ChunkSize = default 64MB

1MB <= ChunkSize <= 1024MB

ChunkSize is configurable in runtime

MinKey ------- Key Space ------- MasKey


Jumbo Chunks

- Larger than the defined chunk size
- Cannot move jumbo chunks
  - Once marked as jumbo the balancer skips these
    chunks and avoids trying to move them
- In some cases these will not be able to be split



################################################################
Show collections in config database:
use config
show collections

Find one document from the chunks collection:
db.chunks.findOne()
{
	"_id" : "config.system.sessions-_id_MinKey",
	"ns" : "config.system.sessions",
	"min" : {
		"_id" : { "$minKey" : 1 }
	},
	"max" : {
		"_id" : { "$maxKey" : 1 }
	},
	"shard" : "m103-repl",
	"lastmod" : Timestamp(1, 0),
	"lastmodEpoch" : ObjectId("5cc66e3cc6ac95dfa4203745")
}

Change the chunk size:
use config
db.settings.save({_id: "chunksize", value: 2})

Check the status of the sharded cluster:
sh.status()
...
databases:
...
        {  "_id" : "m103",  "primary" : "m103-repl",  "partitioned" : true }
                m103.products
                        shard key: { "name" : 1 }
                        unique: false
                        balancing: true
                        chunks:
                                m103-repl	2
                                m103-repl-2	1
                        { "name" : { "$minKey" : 1 } } -->> { "name" : "In Meiner Mitte - CD" } on : m103-repl-2 Timestamp(2, 0) 
                        { "name" : "In Meiner Mitte - CD" } -->> { "name" : "Tha Shiznit: Episode 1 - CD" } on : m103-repl Timestamp(2, 1) 
                        { "name" : "Tha Shiznit: Episode 1 - CD" } -->> { "name" : { "$maxKey" : 1 } } on : m103-repl Timestamp(1, 2) 




Download the attached handout products.part2.json and copy this file to your Vagrant shared folder:
cp ~/Downloads/products.part2.json m103-vagrant-env/shared/.

Import a new dataset (note that this dataset does not exist on the course VM):
mongoimport -d m103 -c products /shared/products.part2.json


#RECAP

- Logical groups of documents
- Chunk can only live at one designated shard at a time
- Shard key cardinality/Frequency, chunk size wil determine number of chunks


################################################################

# Correct answers:

Chunk ranges have an inclusive minimum and an exclusive maximum.
  This is how chunk ranges are defined in MongoDB.

Increasing the maximum chunk size can help eliminate jumbo chunks.
  By definition, jumbo chunks are larger than the maximum chunk size, so raising the max chunk size can get rid of them.

# Incorrect answers:

Chunk ranges can never change once they are set.
  The cluster may split chunks once they become too big.

Documents in the same chunk may live on different shards.
  Individual chunks always remain on the same shard.

Jumbo chunks can be migrated between shards.
  Jumbo chunks cannot be migrated to another shard, which is part of the reason why they are so problematic.


################################################################
# BALANCING

Mongodb identify if a shard has much chunks and automaticaly move shunks across shards

Balancer process run in config servers, it could migrate chunks in parallel 

A given shard cannot participate of more than one migration at time
in 'n' shards divide it by two and you get the number of chunks that can be migrate in a Balacer Round

For instance in a topology with 3 shards, we can migrate 1 chunk at time (3 / 2 ~ 1)

The Balancer Rounds occour in sequence until all chunks are balanced and cluster is evenly distributed as possible

Beyound Mongos, balancer could split chunks if necessary


################################################################
Lecture Instructions
# https://docs.mongodb.com/manual/tutorial/manage-sharded-cluster-balancer/#sharding-schedule-balancing-window

Start the balancer:
sh.startBalancer(timeout, interval)

Stop the balancer:
sh.stopBalancer(timeout, interval)

Enable/disable the balancer:
sh.setBalancerState(boolean)

#RECAP
- Balancer responsible for evenly distribuiting chunks across the sharded cluster
- Balancer runs on Primary member of config server replica set
- Balancer is an automatic process and requires minimal user configuration 


################################################################
Queries in a Sharded Cluster


https://docs.mongodb.com/manual/core/aggregation-pipeline-sharded-collections/

You can read more about routing Aggregation queries in a sharded cluster on the MongoDB sharding docs.

At 2:17, we describe how limit() and skip() are handled in a sharded cluster. There is a specific case not mentioned.

When used in conjunction with a limit(), the mongos will pass the limit plus the value of the skip() 
to the shards to ensure a sufficient number of documents are returned to the mongos to apply 
the final limit() and skip() successfully.

Sort, Limit, and Skip in sharded clusters

sort()
  The mongos pushes the sort to each shard and merge-sorts the results

limit()
  The mongos passes the limit to each targeted shard, then re-applies the limit to the merged set of results

skip()
  The mongos performs the skip against the merged set of results

# RECAP
- mongos handles all queries in the cluster
- mongos builds a list of shards to target a Query
- mongos merges the results from each shard
- mongos supports standard query modifiers like sort, limit, and skip

################################################################
Routed Queries vs Scatter Gather: Part 1

Compound shard key: { "sku": 1, "type": 1, "name": 1 }


Targetable query, can use any of these below:
db.products.find( { "sku" : ... } )
db.products.find( { "sku" : ..., "type" : ... } )
db.products.find( { "sku" : ..., "type" : ..., "name" : ... } )

Scatter-gather queries, you can't perform targetable query, because is not included
the full prefix leading up to either of these fields 
db.products.find( { "type" : ... } )
db.products.find( { "name" : ... } )



################################################################
Routed Queries vs Scatter Gather: Part 2

use m103
show collections

Routed query with explain() output:
db.products.find({"sku" : 1000000749 }).explain()
{
	"queryPlanner" : {
		"mongosPlannerVersion" : 1,
		"winningPlan" : {
			"stage" : "SINGLE_SHARD",             # SINGLE SHARD!!!!!
			"shards" : [
				{
					"shardName" : "m103-repl",        # SINGLE SHARD!!!!!
					"connectionString" : "m103-repl/192.168.103.100:27001,192.168.103.100:27003,m103:27002",
					"serverInfo" : {
						"host" : "m103",
						"port" : 27001,
						"version" : "3.6.12",
						"gitVersion" : "c2b9acad0248ca06b14ef1640734b5d0595b55f1"
					},
					"plannerVersion" : 1,
					"namespace" : "m103.products",
					"indexFilterSet" : false,
					"parsedQuery" : {
						"sku" : {
							"$eq" : 1000000749
						}
					},
					"winningPlan" : {
						"stage" : "FETCH",
						"inputStage" : {
							"stage" : "SHARDING_FILTER",  # SINGLE SHARD!!!!!
							"inputStage" : {
								"stage" : "IXSCAN",
								"keyPattern" : {
									"sku" : 1
								},
								"indexName" : "sku_1",
								"isMultiKey" : false,
								"multiKeyPaths" : {
									"sku" : [ ]
								},
								"isUnique" : false,
								"isSparse" : false,
								"isPartial" : false,
								"indexVersion" : 2,
								"direction" : "forward",
								"indexBounds" : {
									"sku" : [
										"[1000000749.0, 1000000749.0]"
									]
								}
							}
						}
					},
					"rejectedPlans" : [ ]
				}
			]
		}
	},
	"ok" : 1,
	"operationTime" : Timestamp(1556556382, 1),
	"$clusterTime" : {
		"clusterTime" : Timestamp(1556556383, 1),
		"signature" : {
			"hash" : BinData(0,"3QdgEI+hSQhp3FhVNpsB2XxFDqo="),
			"keyId" : NumberLong("6685145540008607770")
		}
	}
}

################################################################
Scatter gather query with explain() output:
db.products.find( {
  "name" : "Gods And Heroes: Rome Rising - Windows [Digital Download]" }
).explain()
{
	"queryPlanner" : {
		"mongosPlannerVersion" : 1,
		"winningPlan" : {
			"stage" : "SHARD_MERGE",          # MERGED SHARD!!!!!
			"shards" : [
				{
					"shardName" : "m103-repl",    # SHARD ONE!!!!!
					"connectionString" : "m103-repl/192.168.103.100:27001,192.168.103.100:27003,m103:27002",
					"serverInfo" : {
						"host" : "m103",
						"port" : 27001,
						"version" : "3.6.12",
						"gitVersion" : "c2b9acad0248ca06b14ef1640734b5d0595b55f1"
					},
					"plannerVersion" : 1,
					"namespace" : "m103.products",
					"indexFilterSet" : false,
					"parsedQuery" : {
						"name" : {
							"$eq" : "Gods And Heroes: Rome Rising - Windows [Digital Download]"
						}
					},
					"winningPlan" : {
						"stage" : "SHARDING_FILTER",
						"inputStage" : {
							"stage" : "COLLSCAN",
							"filter" : {
								"name" : {
									"$eq" : "Gods And Heroes: Rome Rising - Windows [Digital Download]"
								}
							},
							"direction" : "forward"
						}
					},
					"rejectedPlans" : [ ]
				},
				{
					"shardName" : "m103-repl-2",  # SHARD TWO!!!!!
					"connectionString" : "m103-repl-2/192.168.103.100:27004,192.168.103.100:27005,192.168.103.100:27006",
					"serverInfo" : {
						"host" : "m103",
						"port" : 27004,
						"version" : "3.6.12",
						"gitVersion" : "c2b9acad0248ca06b14ef1640734b5d0595b55f1"
					},
					"plannerVersion" : 1,
					"namespace" : "m103.products",
					"indexFilterSet" : false,
					"parsedQuery" : {
						"name" : {
							"$eq" : "Gods And Heroes: Rome Rising - Windows [Digital Download]"
						}
					},
					"winningPlan" : {
						"stage" : "SHARDING_FILTER",
						"inputStage" : {
							"stage" : "COLLSCAN",
							"filter" : {
								"name" : {
									"$eq" : "Gods And Heroes: Rome Rising - Windows [Digital Download]"
								}
							},
							"direction" : "forward"
						}
					},
					"rejectedPlans" : [ ]
				}
			]
		}
	},
	"ok" : 1,
	"operationTime" : Timestamp(1556556418, 1),
	"$clusterTime" : {
		"clusterTime" : Timestamp(1556556418, 1),
		"signature" : {
			"hash" : BinData(0,"5qs0Nr9ZQylBZv/AKHLF9boqhcs="),
			"keyId" : NumberLong("6685145540008607770")
		}
	}
}

################################################################
both queries return the same document, but first one is faster

db.products.find({"sku" : 1000000749 }).pretty()
{
	"_id" : ObjectId("573f706ff29313caab7d7395"),
	"sku" : 1000000749,
	"name" : "Gods And Heroes: Rome Rising - Windows [Digital Download]",
	"type" : "Software",
	"regularPrice" : 39.95,
	"salePrice" : 39.95,
	"shippingWeight" : "0.01"
}

db.products.find( {   "name" : "Gods And Heroes: Rome Rising - Windows [Digital Download]" } ).pretty()
{
	"_id" : ObjectId("573f706ff29313caab7d7395"),
	"sku" : 1000000749,
	"name" : "Gods And Heroes: Rome Rising - Windows [Digital Download]",
	"type" : "Software",
	"regularPrice" : 39.95,
	"salePrice" : 39.95,
	"shippingWeight" : "0.01"
}


################################################################
#RECAP

- Targeted queries require the shard key in the query
- Ranged queries on the shard key may still require targeting every shard in the cluster
- Without the sharded key, the mongos must perform a scatter-gather query


################################################################