Shardes Cluster

Mongos process, use metadata in config servers replica set


Elements

- Shards, store distributed collections
- Config Server, store metadata about each shard
- Mongos, routes queriews to shards


################################################################
When to Shard

do not shard if...
- is possible to impruve current servers
- 10x RAM do not cost 100x $

the dead end...
Horizontal Scale

usually servers has between 2TB to 5TB of data

Sharding offers:
1 - Single Thread Operations
    - Aggregation Pipeline Commands, be  careful about pipeline!
    
2 - Geographical Distributed Data
    Zone Sharding (out of scope)

################################################################

Correct answers:

When we reach the most powerful servers available, maximizing our vertical scale options.
    Sharding can provide an alternative to vertical scaling.

Data sovereignty laws require data to be located in a specific geography.
    Sharding allows us to store different pieces of data in specific countries or regions.

When holding more than 5TB per server and operational costs increase dramatically.
    Generally, when our deployment reaches 2-5TB per server, we should consider sharding.


Incorrect answers:

When we start a new project with MongoDB.
    We should carefully consider if we need to have a sharding system out of the start. 
    This might be required for some projects, but certainly not always the ideal moment to address scalability needs.

When our server disks are full.
    Maxing out the capacity of our disks is not a reason for sharding. 
    Scaling up might make more sense than to add complexity to our system.

################################################################
Sharding Architecture

                                            |----------->Shard1
Client App ------------> Mongos ------------|----------->Shard2
                                            |----------->Shard3


Names divides by...

Shard1, last names A-J
Shard2, last names K-Q
Shard3, last names R-Z

Could be multiple mongos 

Sharded Cluster metadata, stored in Config Servers


Config Servers
    Shard           Data
    1               A-J
    2               K-Q
    3               R-Z


                                            |----------->Shard1
Client App ------------> Mongos ------------|----------->Shard2
                            |               |----------->Shard3
                            |
                        Config Servers


Config Server keep balance between Shards nodes

In the primary shard are made Aggregation operations

SHARD_MERGE takes place in Mongos


################################################################
Setting Up a Sharded Cluster

Shard1 (three nodes)

CSRS (three nodes)

# csrs_1.conf
sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26001
systemLog:
  destination: file
  path: /var/mongodb/db/csrs1.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs1

################################################################
# csrs_2.conf
sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26002
systemLog:
  destination: file
  path: /var/mongodb/db/csrs2.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs2

################################################################
# csrs_3.conf
sharding:
  clusterRole: configsvr
replication:
  replSetName: m103-csrs
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26003
systemLog:
  destination: file
  path: /var/mongodb/db/csrs3.log
  logAppend: true
processManagement:
  fork: true
storage:
  dbPath: /var/mongodb/db/csrs3


################################################################
# creating db folders

mkdir /var/mongodb/db/{csrs1,csrs2,csrs3}
################################################################
Starting the three config servers:

mongod -f csrs_1.conf
mongod -f csrs_2.conf
mongod -f csrs_3.conf
################################################################
# Connect to one of the config servers
mongo --port 26001

################################################################
# Initiating the CSRS:

rs.initiate()

################################################################
# Creating super user on CSRS:
use admin
db.createUser({
  user: "m103-admin",
  pwd: "m103-pass",
  roles: [
    {role: "root", db: "admin"}
  ]
})
################################################################
# Authenticating as the super user:

db.auth("m103-admin", "m103-pass")

################################################################
# Add the second and third node to the CSRS:

rs.add("192.168.103.100:26002")
rs.add("192.168.103.100:26003")

################################################################
# Connecting to the cluster

mongo --host "m103-csrs/192.168.103.100:26001" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

################################################################
# Connecting to each node

mongo --host "192.168.103.100:26001" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"
mongo --host "192.168.103.100:26002" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"
mongo --host "192.168.103.100:26003" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

################################################################
# Mongos config (mongos.conf)

sharding:
  configDB: m103-csrs/192.168.103.100:26001,192.168.103.100:26002,192.168.103.100:26003
security:
  keyFile: /var/mongodb/pki/m103-keyfile
net:
  bindIp: localhost,192.168.103.100
  port: 26000
systemLog:
  destination: file
  path: /var/mongodb/db/mongos.log
  logAppend: true
processManagement:
  fork: true


################################################################
# run mongos 

mongos -f mongos.conf

################################################################
# Connect to mongos

mongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin

MongoDB shell version v3.6.12
connecting to: mongodb://127.0.0.1:26000/?authSource=admin&gssapiServiceName=mongodb
Implicit session: session { "id" : UUID("2307e517-aa9b-4629-83cb-7f0fdeb1d361") }
MongoDB server version: 3.6.12


################################################################
# Check sharding status:

sh.status()
--- Sharding Status --- 
  sharding version: {
  	"_id" : 1,
  	"minCompatibleVersion" : 5,
  	"currentVersion" : 6,
  	"clusterId" : ObjectId("5cc62d5eb15caac4aa6fbe93")
  }
  shards:
  active mongoses:
        "3.6.12" : 1
  autosplit:
        Currently enabled: yes
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours: 
                No recent migrations
  databases:
        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }

################################################################
# Updated configuration for node1.conf:

sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node1
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27011
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node1/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

################################################################
# Updated configuration for node2.conf:

sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node2
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27012
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node2/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

################################################################
# Updated configuration for node3.conf:

sharding:
  clusterRole: shardsvr
storage:
  dbPath: /var/mongodb/db/node3
  wiredTiger:
    engineConfig:
      cacheSizeGB: .1
net:
  bindIp: 192.168.103.100,localhost
  port: 27013
security:
  keyFile: /var/mongodb/pki/m103-keyfile
systemLog:
  destination: file
  path: /var/mongodb/db/node3/mongod.log
  logAppend: true
processManagement:
  fork: true
replication:
  replSetName: m103-example

################################################################
# Starting nodes from replicaSet folder

cd ../replicaSet/
mongod -f node1.conf
mongod -f node2.conf
mongod -f node3.conf

# Checkig it
mongo --host "m103-example/192.168.103.100:27011" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

# come back to shardedCluster folder
cd ../shardedCluster/

################################################################
# Connecting directly to secondary node (note that if an election has taken place in your replica set, 
the specified node may have become primary):

# connect to a secondary node

mongo --port 27012 -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"


################################################################
# Shutting down node:
use admin
db.shutdownServer()

################################################################
# Restarting node with new configuration:
mongod -f node2.conf

# do the same with node3
mongo --port 27013 -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

use admin
db.shutdownServer()

mongod -f node3.conf

################################################################
# Connect to the last one 

mongo --port 27011 -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

################################################################
# Stepping down current primary:

rs.stepDown()

# shut it down and run new config
use admin
db.shutdownServer()

mongod -f node1.conf

################################################################
# Connect to the mongos

mongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin

################################################################
# Adding new shard to cluster from mongos:

sh.addShard("m103-example/192.168.103.100:27013")

{
	"shardAdded" : "m103-example",
	"ok" : 1,
	"operationTime" : Timestamp(1556498029, 4),
	"$clusterTime" : {
		"clusterTime" : Timestamp(1556498029, 4),
		"signature" : {
			"hash" : BinData(0,"4D3z83YxpGPuRy/12MSJuZWR6cU="),
			"keyId" : NumberLong("6685080578628255770")
		}
	}
}


################################################################
# checking status

--- Sharding Status --- 
  sharding version: {
  	"_id" : 1,
  	"minCompatibleVersion" : 5,
  	"currentVersion" : 6,
  	"clusterId" : ObjectId("5cc62d5eb15caac4aa6fbe93")
  }
  shards:
        {  "_id" : "m103-example",  "host" : "m103-example/192.168.103.100:27011,192.168.103.100:27012,192.168.103.100:27013",  "state" : 1 }
  active mongoses:
        "3.6.12" : 1
  autosplit:
        Currently enabled: yes
  balancer:
        Currently enabled:  yes
        Currently running:  no
        Failed balancer rounds in last 5 attempts:  0
        Migration Results for the last 24 hours: 
                No recent migrations
  databases:
        {  "_id" : "config",  "primary" : "config",  "partitioned" : true }


################################################################

mongo --host "m103-example/192.168.103.100:27011" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"
mongo --host "192.168.103.100:27011" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

