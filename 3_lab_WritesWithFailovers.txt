# In this lab, you will attempt to write data with a writeConcern to a replica set where one node has failed.
# In order to simulate a node failure within your replica set, you will connect to the node individually and shut it down. 
# Connecting back to the replica set and running rs.status() should show the failing node with a description like this:

{
  "name" : "m103:27001",
  "health" : 0,
  "stateStr" : "(not reachable/healthy)",
  "lastHeartbeatMessage" : "Connection refused",
  "configVersion" : -1
}

# With one of your nodes down, attempt to insert a document in your replica set by running the following commands:

use testDatabase
db.new_data.insert({"m103": "very fun"}, { writeConcern: { w: 3, wtimeout: 1000 }})

# This will attempt to insert one record into a collection called testDatabase.new_data, 
# while verifying that 3 nodes registered the write. It should return an error, because only 2 nodes are healthy.

@ Given the output of the insert command, and your knowledge of writeConcern, check all that apply:

# connect to the replica set
mongo --host "m103-repl/192.168.103.100:27001" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

# check nodes
PRIMARY> rs.isMaster()
{
	"hosts" : [
		"192.168.103.100:27001",
		"192.168.103.100:27003",
		"m103:27002"
	],
	"setName" : "m103-repl",
	"setVersion" : 7,
	"ismaster" : true,
	"secondary" : false,
	"primary" : "192.168.103.100:27001",
	"me" : "192.168.103.100:27001",
...

# shutdown primary
use admin
db.shutdownServer()

# the new primary is m103:27002
PRIMARY> rs.isMaster()
{
	"hosts" : [
		"192.168.103.100:27001",
		"192.168.103.100:27003",
		"m103:27002"
	],
	"setName" : "m103-repl",
	"setVersion" : 7,
	"ismaster" : true,
	"secondary" : false,
	"primary" : "m103:27002",
	"me" : "m103:27002",
...

# and the former primary are down
PRIMARY> rs.status()
...
	"members" : [
		{
			"_id" : 0,
			"name" : "192.168.103.100:27001",
			"health" : 0,
			"state" : 8,
			"stateStr" : "(not reachable/healthy)",
...

# perform the insertion
PRIMARY> use testDatabase
PRIMARY> db.new_data.insert({"m103": "very fun"}, { writeConcern: { w: 3, wtimeout: 1000 }})
WriteResult({
	"nInserted" : 1,
	"writeConcernError" : {
		"code" : 64,
		"codeName" : "WriteConcernFailed",
		"errInfo" : {
			"wtimeout" : true
		},
		"errmsg" : "waiting for replication timed out"
	}
})

# data was inserted in primary node

PRIMARY> use testDatabase
switched to db testDatabase
PRIMARY> db.new_data.find()
{ "_id" : ObjectId("5cbe46995dc99faf9d85c055"), "m103" : "very fun" }

# connecting in the secondary
mongo --host "192.168.103.100:27003" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

# and data is there
SECONDARY> rs.slaveOk()
SECONDARY> use testDatabase
switched to db testDatabase
SECONDARY> db.new_data.find()
{ "_id" : ObjectId("5cbe46995dc99faf9d85c055"), "m103" : "very fun" }

# running former primary agains
mongod -f mongod-repl-1.conf

# checking that it is online
PRIMARY> rs.status()
...
"members" : [
		{
			"_id" : 0,
			"name" : "192.168.103.100:27001",
			"health" : 1,
			"state" : 2,
			"stateStr" : "SECONDARY",
			"uptime" : 39,
...

# connecting to the resurrected node
mongo --host "192.168.103.100:27001" -u "m103-admin" -p "m103-pass" --authenticationDatabase "admin"

# and data is there too!
SECONDARY> rs.slaveOk()
SECONDARY> use testDatabase
switched to db testDatabase
SECONDARY> db.new_data.find()
{ "_id" : ObjectId("5cbe46995dc99faf9d85c055"), "m103" : "very fun" }



# testing majority

# shutdown primary
use admin
db.shutdownServer()

use testDatabase
db.new_data.insert({"m103": "very fun"}, { writeConcern: { w: "majority", wtimeout: 1000 }})
WriteResult({ "nInserted" : 1 })

#####################################################
Correct:

- When a writeConcernError occurs, the document is still written to the healthy nodes.
        The WriteResult object simply tells us whether the writeConcern was successful or not 
        - it will not undo successful writes from any of the nodes.

- The unhealthy node will have the inserted document when it is brought back online.
        When the unhealthy node comes back online, it rejoins the replica set and its oplog is compared 
        with the other nodes' oplogs. Any missing operations will be replayed on the newly healthy node.

#####################################################
Incorrect:

- w: "majority" would also cause this write operation to return with an error.
        w: "majority" requests acknowledgement that a majority of nodes in a replica set have registered the write. 
        In a three-node replica set, only two nodes are required for a majority, so the two healthy nodes are sufficient 
        to satisfy this writeConcern.

- The write operation will always return with an error, even if wtimeout is not specified.
        If wtimeout is not specified, the write operation will be retried for an indefinite amount of time 
        until the writeConcern is successful. If the writeConcern is impossible, like in this example, 
        it may never return anything to the client.
